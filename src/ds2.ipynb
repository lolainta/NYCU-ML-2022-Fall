{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis On Movie Reviews\n",
    "\n",
    "In this problem, we are given a dataset of movie reviews and the corresponding sentiment (positive or negative). Our goal is to build a model that can classify a movie review as positive or negative based on its text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from classifiers.RandomClassifier import RandomClassifier \n",
    "from classifiers.DecisionTreeClassifier import DecisionTreeClassifier\n",
    "from StatisticManager import StatisticManager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and show the head of the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw=pandas.read_excel('../data/Dataset2_train/X_train.xlsx')\n",
    "y_train_raw=pandas.read_excel('../data/Dataset2_train/y_train.xlsx')\n",
    "X_train_raw.info()\n",
    "X_train_raw.head()\n",
    "y_train_raw.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup the target feature column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"Sentiment\"\n",
    "# target=\"class\"\n",
    "target_feature=target+\"_cat\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill all missing values with the median of that descriptive feature\n",
    "- Change the target feature to an catagorical feature\n",
    "- Show the percentage of each classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw.fillna('',inplace=True)\n",
    "\n",
    "\n",
    "y_train_raw[target_feature]=y_train_raw[target].astype(\"category\")\n",
    "y_train_raw.drop([target],axis=1,inplace=True)\n",
    "y_train_raw.info()\n",
    "y_train_raw[target_feature].value_counts(normalize=True)*100\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Profile the descriptive features of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# ProfileReport(X_train,title=\"X_train profiling\")\n",
    "# ProfileReport(y_train,title=\"y_train profiling\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw['len']=X_train_raw['Phrase'].apply(len)\n",
    "X_train_raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive\n",
    "This section in not used for the final result. Please skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q=X_train_raw.Phrase[4]\n",
    "# word_tokenize(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer=SnowballStemmer(language='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return [token.lower() for token in word_tokenize(text) if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# stopWords=stopwords.words('english')\n",
    "# \", \".join(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer=TfidfVectorizer(tokenizer=tokenize,lowercase=True,stop_words=stopWords,ngram_range=(1,2),max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs=vectorizer.fit_transform(X_train_raw.Phrase)\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(inputs)\n",
    "# vectorizer.get_feature_names_out()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.fit(X_train_raw.Phrase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert all token into lower cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.vocabulary_\n",
    "# vectorizer.get_feature_names_out()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs=vectorizer.transform(X_train_raw.Phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree \n",
    "- Reference to the demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    reviews = []\n",
    "    for raw in tqdm.tqdm(df['Phrase']):\n",
    "        text = BeautifulSoup(raw,'lxml').get_text()\n",
    "        only_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        words = word_tokenize(only_text.lower())\n",
    "        stops = set(stopwords.words('english'))\n",
    "        non_stopwords = [word for word in words if not word in stops]\n",
    "        lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]    \n",
    "        reviews.append(lemma_words)\n",
    "    return reviews\n",
    "\n",
    "def tokenizer_preprocess(list_X_train, list_X_val):\n",
    "    unique_words = set()\n",
    "    len_max = 0\n",
    "    for sent in tqdm.tqdm(list_X_train):\n",
    "        unique_words.update(sent)\n",
    "        if len_max < len(sent):\n",
    "            len_max = len(sent)\n",
    "    len(list(unique_words)), len_max\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "    tokenizer.fit_on_texts(list(list_X_train))\n",
    "     \n",
    "    X_train = tokenizer.texts_to_sequences(list_X_train)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "\n",
    "    X_val = tokenizer.texts_to_sequences(list_X_val)\n",
    "    X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "\n",
    "    return X_train, X_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data\n",
    "\n",
    "Since the original dataset is too large, it will take too long to train the model. Thus, we can randomly sample somedata to reduce the training process. However, this may also decrease the performance of the model.\n",
    "\n",
    "To save your time, it is recommanded to use 10% of data or less. You can change the parameter easily at the `randint` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train_raw)==len(y_train_raw), \"Length error\"\n",
    "size=len(X_train_raw)\n",
    "\n",
    "rows=np.random.randint(10,size=size).astype('bool')\n",
    "print(rows)\n",
    "X_train_raw=X_train_raw[~rows]\n",
    "y_train_raw=y_train_raw[~rows]\n",
    "print(len(X_train_raw))\n",
    "print(X_train_raw.index)\n",
    "print(y_train_raw.index)\n",
    "\n",
    "# print(idx.index)\n",
    "# X_train_raw=X_train_raw.loc[idx.index]\n",
    "# y_train_raw=y_train_raw.loc[idx.index]\n",
    "# print(X_data.index)\n",
    "# print(y_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_data=np.array(preprocess_data(X_train_raw))\n",
    "y_data=y_train_raw.values\n",
    "print(type(X_data),type(y_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRawData(X,idx):\n",
    "    vocabularies=set()\n",
    "    for i in idx:\n",
    "        for voc in X[i]:\n",
    "            vocabularies.add(voc)\n",
    "    print(f\"{len(vocabularies)} vocabularies found\")\n",
    "\n",
    "    data=dict()\n",
    "    for voc in tqdm.tqdm(vocabularies):\n",
    "        data[voc]=[X[i].count(voc) for i in idx]\n",
    "        assert len(data[voc])==len(idx),\"Internall error\"\n",
    "    return pandas.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifiers.DecisionTreeClassifier import DecisionTreeClassifier\n",
    "from StatisticManager import StatisticManager\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf=KFold(n_splits=10,random_state=72510,shuffle=True)\n",
    "kf.get_n_splits(X_data)\n",
    "print(kf)\n",
    "import time\n",
    "for i,(train_idx,test_idx) in enumerate(kf.split(X_data)):\n",
    "    print(\"==================================================\")\n",
    "    print(f\"Fold {i}: {len(train_idx)}:{len(test_idx)}\")\n",
    "    # print(f\"Train: index={train_idx}\")\n",
    "    # print(f\"Test:  index={test_idx}\")\n",
    "\n",
    "    X_train=convertRawData(X_data,train_idx)\n",
    "    y_train=pandas.DataFrame(y_data[train_idx],columns=[target_feature])\n",
    "\n",
    "    X_test=convertRawData(X_data,test_idx)\n",
    "    y_test=pandas.DataFrame(y_data[test_idx],columns=[target_feature])\n",
    "\n",
    "    # X_train.info()\n",
    "    # X_test.info()\n",
    "    # y_train.info()\n",
    "    # y_test.info()\n",
    "\n",
    "    clf=DecisionTreeClassifier(depth=3,target_feature=target_feature)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "\n",
    "    statistic=StatisticManager(target_feature)\n",
    "    statistic.report(y_pred,y_test)\n",
    "    # statistic.evaluate(clf,X_train,y_train)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a4f55d9e123273fd62cd91d79c68cbedd3309d0ae89ea8c5527d28786b75bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
